{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets evaluate matplotlib sentencepiece\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "bc5fcd363e8ff611",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------------- CONFIG --------------------------------\n",
    "input_file = \"../Processed_MS_SDoH_pubmed_abstracts_with_entities.json\"\n",
    "train_output_file = \"train.json\"\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "llama_model_id = \"meta-llama/Llama-3.2-1B\"  # Replace with a real model on HuggingFace\n",
    "ALLOWED_RELATIONS = [\"risk_factor_for\", \"protective_against\", \"associated_with\", \"no_relation\"]\n",
    "\n",
    "# Set up spaCy and NLTK\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt')"
   ],
   "id": "2875071834e3fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------------- STEP 1: LOAD DATA --------------------------------\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Define a function to check if an entity is biomedical.\n",
    "# For now, we consider 'Disease_disorder' as biomedical.\n",
    "# If you have other categories like 'Medication' (drugs) or 'Biological_structure' (genes),\n",
    "# you can add them to the conditions.\n",
    "def is_biomedical_entity(e):\n",
    "    if e.get('entity_group') in ['Disease_disorder']:\n",
    "        return True\n",
    "    # Add more conditions for other biomedical categories if needed.\n",
    "    return False\n",
    "\n",
    "def extract_candidates(article):\n",
    "    abstract = article.get('abstract', '')\n",
    "    if not abstract:\n",
    "        return []\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    \n",
    "    # Extract biomedical entities (e.g., diseases)\n",
    "    biomedical_entities = [e for e in article.get('entities', []) if is_biomedical_entity(e)]\n",
    "    # Extract SDoH mentions\n",
    "    sdoh_entities = article.get('sdoh_mentions', [])\n",
    "\n",
    "    candidates = []\n",
    "    for sent in sentences:\n",
    "        sent_lower = sent.lower()\n",
    "        # Find any SDoH mention in this sentence\n",
    "        for sdoh in sdoh_entities:\n",
    "            sdoh_text = sdoh['phrase']\n",
    "            if sdoh_text.lower() in sent_lower:\n",
    "                # If SDoH found, check if there's a biomedical entity\n",
    "                for bio in biomedical_entities:\n",
    "                    # The biomedical entity text may be composed from pieces (e.g., \"multiple\" + \"##osis\").\n",
    "                    # In the example, 'word' is a piece of text. If the model splits words like \"multiple sclerosis\"\n",
    "                    # into \"multiple\" and \"##osis\", you might need to reconstruct or rely on start/end offsets.\n",
    "                    # For simplicity, we will just use the 'word' field as is, but note it might not represent a full entity.\n",
    "                    bio_text = bio['word']\n",
    "                    # Because some entities might be split (e.g., \"multiple\", \"##osis\"), we should normalize:\n",
    "                    # Combine split tokens if they belong to same entity by checking start/end indexes.\n",
    "                    # For a simplified demonstration, we assume 'word' is either a complete entity or just use it as a keyword.\n",
    "                    \n",
    "                    # If you have multiple pieces for one entity, you'd need to merge them. For the example provided:\n",
    "                    # \"multiple\" and \"##osis\" represent \"multiple sclerosis\".\n",
    "                    # A quick hack: if entity_group == Disease_disorder and we see \"multiple\" and the next is \"##osis\",\n",
    "                    # combine them into \"multiple sclerosis\".\n",
    "                    # This is a heuristic just for demonstration:\n",
    "                    if bio.get('entity_group') == 'Disease_disorder':\n",
    "                        # Check if next token is \"##osis\" to form \"multiple sclerosis\"\n",
    "                        # In reality, you'd need a more robust method if multiple tokens form one entity.\n",
    "                        # For now, let's just handle the case found in the example.\n",
    "                        if bio_text == \"multiple\":\n",
    "                            # Look ahead for \"##osis\"\n",
    "                            # We know entities are a list, find next entity starting right after 'multiple'\n",
    "                            idx = article['entities'].index(bio)\n",
    "                            if idx + 1 < len(article['entities']):\n",
    "                                next_e = article['entities'][idx+1]\n",
    "                                if next_e['word'] == \"##osis\":\n",
    "                                    bio_text = \"multiple sclerosis\"\n",
    "                    \n",
    "                    if bio_text.lower() in sent_lower:\n",
    "                        candidates.append((sent, sdoh_text, bio_text))\n",
    "    return candidates\n",
    "\n",
    "all_candidates = []\n",
    "for article in articles:\n",
    "    cands = extract_candidates(article)\n",
    "    for (sent, sdoh_text, bio_text) in cands:\n",
    "        all_candidates.append({\n",
    "            \"sentence\": sent,\n",
    "            \"sdoh\": sdoh_text,\n",
    "            \"biomedical_entity\": bio_text\n",
    "        })\n",
    "\n",
    "print(\"Extracted candidate sentences with both SDoH and biomedical entities:\")\n",
    "for c in all_candidates[:5]:\n",
    "    print(c)\n",
    "# If too large, sample a subset for demonstration\n",
    "all_candidates = all_candidates[:200]"
   ],
   "id": "c0ebebb6442f5686",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "## Suppress transformers warnings (including pad_token_id messages)\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"Starting LLM-based relation annotation...\")\n",
    "\n",
    "# -------------------------------- STEP 2: USE LLM TO ANNOTATE RELATIONS --------------------------------\n",
    "# We'll prompt Llama to choose one of ALLOWED_RELATIONS.\n",
    "llm_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama_model_id,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "def llm_classify_relation(sentence, sdoh_text, bio_text):\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical relation extraction assistant. We have a set of possible relationships: {ALLOWED_RELATIONS}.\n",
    "Given the sentence: \"{sentence}\"\n",
    "The first entity (SDoH) is: \"{sdoh_text}\"\n",
    "The second entity (Biomedical) is: \"{bio_text}\"\n",
    "\n",
    "Determine the best fitting relationship from {ALLOWED_RELATIONS}, or 'no_relation' if none applies.\n",
    "\n",
    "Just output the relation label exactly.\n",
    "\"\"\"\n",
    "    response = llm_pipe(prompt)[0]['generated_text'].strip().lower()\n",
    "    for rel in ALLOWED_RELATIONS:\n",
    "        if rel in response:\n",
    "            return rel\n",
    "    return \"no_relation\"\n",
    "\n",
    "training_data = []\n",
    "\n",
    "# Use tqdm for a progress bar during the annotation process\n",
    "for ex in tqdm(all_candidates, desc=\"Annotating relations\", unit=\"example\"):\n",
    "    rel = llm_classify_relation(ex[\"sentence\"], ex[\"sdoh\"], ex[\"biomedical_entity\"])\n",
    "    training_data.append({\n",
    "        \"text\": ex[\"sentence\"],\n",
    "        \"entity1\": ex[\"sdoh\"],\n",
    "        \"entity2\": ex[\"biomedical_entity\"],\n",
    "        \"label\": rel\n",
    "    })\n",
    "\n",
    "with open(train_output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Training data saved to {train_output_file}\")"
   ],
   "id": "f33ebe51f6e0e851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------------- STEP 3: CREATE TRAIN/VAL SPLIT --------------------------------\n",
    "# Split the generated data into train/val\n",
    "random.shuffle(training_data)\n",
    "split_idx = int(0.8 * len(training_data))\n",
    "train_data = training_data[:split_idx]\n",
    "val_data = training_data[split_idx:]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})"
   ],
   "id": "18b236456581185e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------------- STEP 4: FINE-TUNE BIOBERT ON NEW DATA --------------------------------\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(ALLOWED_RELATIONS)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def mark_entities(example):\n",
    "    text = example[\"text\"]\n",
    "    e1 = example[\"entity1\"]\n",
    "    e2 = example[\"entity2\"]\n",
    "    start_e1 = text.lower().find(e1.lower())\n",
    "    start_e2 = text.lower().find(e2.lower())\n",
    "    if start_e1 == -1 or start_e2 == -1:\n",
    "        # fallback: no markers if entities not found\n",
    "        marked_text = text\n",
    "    else:\n",
    "        # ensure order\n",
    "        if start_e2 < start_e1:\n",
    "            (start_e1, start_e2) = (start_e2, start_e1)\n",
    "            (e1, e2) = (e2, e1)\n",
    "        end_e1 = start_e1 + len(e1)\n",
    "        end_e2 = start_e2 + len(e2)\n",
    "        marked_text = (\n",
    "            text[:start_e1] + \"[E1]\" + text[start_e1:end_e1] + \"[/E1]\" +\n",
    "            text[end_e1:start_e2] + \"[E2]\" + text[start_e2:end_e2] + \"[/E2]\" +\n",
    "            text[end_e2:]\n",
    "        )\n",
    "    return {\"marked_text\": marked_text}\n",
    "\n",
    "dataset = dataset.map(mark_entities)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # batch[\"marked_text\"] is a list of strings\n",
    "    # Run tokenizer in batched mode\n",
    "    result = tokenizer(\n",
    "        batch[\"marked_text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Convert each label in the batch to an ID\n",
    "    result[\"labels\"] = [label2id[l] for l in batch[\"label\"]]\n",
    "    return result\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\", \"entity1\", \"entity2\", \"label\", \"marked_text\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(ALLOWED_RELATIONS))\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "batch_size = 8\n",
    "logging_steps = 10\n",
    "output_dir = \"./relation_extraction_model\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=logging_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "trainer.save_model(output_dir)"
   ],
   "id": "9642f29cc3f09f4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ------------------------- STEP 5: PLOT THE LOSS -------------------------\n",
    "log_history = trainer.state.log_history\n",
    "losses = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\n",
    "steps = range(len(losses))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(steps, losses, label='Training Loss')\n",
    "plt.xlabel('Logging Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"training_loss_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete. Model saved and training loss plot generated.\")\n"
   ],
   "id": "f87c55a8ceaddb85",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
